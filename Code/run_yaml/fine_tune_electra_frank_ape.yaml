description: Electra-large-frank-ape-original-code

target:
  service: amlk8s
  # run "amlt target list amlk8s" to list the names of available AMLK8s targets
  name: itpscusv100cl #itplabrr1cl1
  vc: resrchvc

environment:
  image: pytorch/pytorch:1.5-cuda10.1-cudnn7-devel # pytorch/pytorch:1.0.1-cuda10.0-cudnn7-devel, 1.4-cuda10.1-cudnn7-devel, 1.6.0-cuda10.1-cudnn7-devel
  setup:
    - pip install -r requirements.txt
    - pip install --user boto3
    - pip install --user regex
    - pip install --user sklearn
    - pip install --user tensorflow-gpu==2.3.1
    # - git clone https://github.com/NVIDIA/apex
    # - cd apex
    # - pip install -v --disable-pip-version-check --no-cache-dir --global-option="--cpp_ext" --global-option="--cuda_ext" ./

code:
  # local directory of the code. this will be uploaded to the server.
  # $CONFIG_DIR is expanded to the directory of this config file
  local_dir: C:\Users\ldelcorro\source\repos\autodistil\Code

data:
  local_dir: C:\Users\ldelcorro\source\repos\autodistil
  remote_dir: wikidata

storage:
  output:
    storage_account_name: lucianoblobstorage
    container_name: blobcontainer

search:
  job_template:
    name: Task_{TASK_NAME}_Layer_{Depth_Mult_List}_Hid_{Hidden_Mult_List}_Head_{Width_Mult_List}_Ratio_{Intermediate_Mult_List}_original_code
    sku: G1
    command:
      - python finetune_glue.py
        --model_type bert
        --task_name {TASK_NAME}
        --do_train
        --data_dir $$PT_DATA_DIR/Data_GLUE/glue_data/{TASK_NAME}/
        --model_dir  $$PT_DATA_DIR/Local_models/custom_models/subho-pretrained-model-run-frank-ape-step-400000/
        --output_dir $$PT_DATA_DIR/../projects/kdnas/amlt-results/frank_ape_original_code/Task_{TASK_NAME}_Layer_{Depth_Mult_List}_Hid_{Hidden_Mult_List}_Head_{Width_Mult_List}_Ratio_{Intermediate_Mult_List}
        --max_seq_length {Max_Seq_Length}
        --learning_rate {LR}
        --per_gpu_train_batch_size {Per_GPU_Train_Batch_Size}
        --num_train_epochs {Num_Train_Epochs}
        --hidden_mult_list {Hidden_Mult_List}
        --depth_mult_list {Depth_Mult_List}
        --width_mult_list {Width_Mult_List}
        --intermediate_mult_list {Intermediate_Mult_List}
        --training_phase {Training_Phase}
  type: grid
  max_trials: 100
  # parallel_trials: 2 ???
  params:
    - name: TASK_NAME
      spec: discrete
      values: ['MNLI']
    - name: Max_Seq_Length
      spec: discrete
      values: [128]
    - name: LR
      spec: discrete
      values: [2e-5]
    - name: Per_GPU_Train_Batch_Size
      spec: discrete
      values: [32]
    - name: Num_Train_Epochs
      spec: discrete
      values: [12]                  # [0.083, 0.167, 0.25, 0.333, 0.417, 0.5, 0.583, 0.667, 0.75 , 0.833, 0.917, 1.0 ]
    - name: Depth_Mult_List                    ## base = H, A, R, L = 32*18, 12, 4.0, 12 = 66,019,968
      spec: discrete
      values: ['0.166667', '0.2083333', '0.25', '0.2916666667'] #['4/24,7/24,4']
    - name: Width_Mult_List
      spec: discrete
      values: ['0.4375', '0.5', '0.5625', '0.625'] #['7/16,10/16,4']
    - name: Hidden_Mult_List
      spec: discrete
      values: ['0.125', '0.15625', '0.1875', '0.21875']  #['4/32,7/32,4']
    - name: Intermediate_Mult_List
      spec: discrete
      values: ['2.0', '2.5', '3.0', '3.5']
    - name: Training_Phase
      spec: discrete
      values: ['final_finetuning']